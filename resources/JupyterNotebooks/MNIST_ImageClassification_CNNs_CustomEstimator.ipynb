{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.3\n",
      "0.20.1\n",
      "1.0.0\n",
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sp.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"mnist_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_digit(digit):\n",
    "    plt.imshow(digit.reshape(28, 28), cmap=\"Greys\", interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114652c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(mnist.train.images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_feature_maps = 32\n",
    "conv1_kernel_size = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_feature_maps = 64\n",
    "conv2_kernel_size = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_feature_maps = conv2_feature_maps\n",
    "\n",
    "n_fullyconn1 = 64\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(features):\n",
    "\n",
    "    X = tf.reshape(features['images'], shape=[-1, height, width, channels])\n",
    "\n",
    "    conv1 = tf.layers.conv2d(X, filters=conv1_feature_maps,\n",
    "                             kernel_size=conv1_kernel_size,\n",
    "                             strides=conv1_stride, padding=conv1_pad,\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1, filters=conv2_feature_maps, \n",
    "                             kernel_size=conv2_kernel_size,\n",
    "                             strides=conv2_stride, padding=conv2_pad,\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.nn.max_pool(conv2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding=\"VALID\")\n",
    "    \n",
    "    \n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_feature_maps * 7 * 7])\n",
    "    \n",
    "    fullyconn1 = tf.layers.dense(pool3_flat, n_fullyconn1,\n",
    "                                 activation=tf.nn.relu)\n",
    "    \n",
    "    logits = tf.layers.dense(fullyconn1, n_outputs)\n",
    "    \n",
    "    return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    logits = build_cnn(features)\n",
    "    \n",
    "    predicted_classes = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    # Prediction mode    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predicted_classes)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Training mode    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # Evaluation mode    \n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predicted_classes,\n",
    "        train_op=train_op,\n",
    "        loss=loss,\n",
    "        eval_metric_ops={'accuracy': accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 2000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/yd/1rlyjfk975d3bb98d7_nyt740000gn/T/tmpOkepMC\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x114753810>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/yd/1rlyjfk975d3bb98d7_nyt740000gn/T/tmpOkepMC', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(cnn_model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/yd/1rlyjfk975d3bb98d7_nyt740000gn/T/tmpOkepMC/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.30638, step = 1\n",
      "INFO:tensorflow:global_step/sec: 7.59427\n",
      "INFO:tensorflow:loss = 0.112845, step = 101 (13.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.52085\n",
      "INFO:tensorflow:loss = 0.10263, step = 201 (13.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.26299\n",
      "INFO:tensorflow:loss = 0.084625, step = 301 (12.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.43335\n",
      "INFO:tensorflow:loss = 0.0516369, step = 401 (11.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.48616\n",
      "INFO:tensorflow:loss = 0.024489, step = 501 (11.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.47863\n",
      "INFO:tensorflow:loss = 0.0782672, step = 601 (11.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.50595\n",
      "INFO:tensorflow:loss = 0.032685, step = 701 (11.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.43757\n",
      "INFO:tensorflow:loss = 0.154804, step = 801 (11.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.47357\n",
      "INFO:tensorflow:loss = 0.143314, step = 901 (11.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.54072\n",
      "INFO:tensorflow:loss = 0.0212437, step = 1001 (11.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.57694\n",
      "INFO:tensorflow:loss = 0.0943276, step = 1101 (11.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.5096\n",
      "INFO:tensorflow:loss = 0.126239, step = 1201 (11.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.512\n",
      "INFO:tensorflow:loss = 0.087019, step = 1301 (11.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.27128\n",
      "INFO:tensorflow:loss = 0.136475, step = 1401 (12.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.44405\n",
      "INFO:tensorflow:loss = 0.0357852, step = 1501 (11.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.48373\n",
      "INFO:tensorflow:loss = 0.00913319, step = 1601 (11.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.45578\n",
      "INFO:tensorflow:loss = 0.00958953, step = 1701 (11.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.48102\n",
      "INFO:tensorflow:loss = 0.116924, step = 1801 (11.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.50821\n",
      "INFO:tensorflow:loss = 0.0138155, step = 1901 (11.754 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/yd/1rlyjfk975d3bb98d7_nyt740000gn/T/tmpOkepMC/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0124613.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x114b1df90>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-12-21-08:15:04\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/yd/1rlyjfk975d3bb98d7_nyt740000gn/T/tmpOkepMC/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-21-08:15:08\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9832, global_step = 2000, loss = 0.0667596\n"
     ]
    }
   ],
   "source": [
    "e = model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
